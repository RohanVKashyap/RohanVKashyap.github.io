@inproceedings{burt2019rates,
 abstract = {Excellent variational approximations to Gaussian process posteriors have been developed which avoid the $\mathcalOłeft(N^3i̊ght)$ scaling with dataset size $N$. They reduce the computational cost to $\mathcalOłeft(NM^2\g̊ht)$, with $Młl N$ the number of inducing variables, which summarise the process. While the computational cost seems to be linear in $N$, the true complexity of the algorithm depends on how $M$ must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, $M=\mathcalO(łog^D N)$ suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase $M$ in continual learning scenarios.},
 author = {Burt, David and Rasmussen, Carl Edward and van der Wilk, Mark},
 booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
 editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
 month = {Jun},
 pages = {862--871},
 pdf = {http://proceedings.mlr.press/v97/burt19a/burt19a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Rates of Convergence for Sparse Variational Gaussian Process Regression},
 url = {http://proceedings.mlr.press/v97/burt19a.html},
 volume = {97},
 year = {2019}
}

