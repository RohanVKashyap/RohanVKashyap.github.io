@inproceedings{ober2021promises,
 abstract = {Deep kernel learning and related techniques promise to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify pathological behavior, including overfitting, on a simple toy example. We explore this pathology, explaining its origins and considering how it applies to real datasets. Through careful experimentation on UCI datasets, CIFAR-10, and the UTKFace dataset, we find that the overfitting from overparameterized deep kernel learning, in which the model is “somewhat Bayesian”, can in certain scenarios be worse than that from not being Bayesian at all. However, we find that a fully Bayesian treatment of deep kernel learning can rectify this overfitting and obtain the desired performance improvements over standard neural networks and Gaussian processes.},
 author = {Ober, Sebastian W. and Rasmussen, Carl E. and van der Wilk, Mark},
 booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence (UAI)},
 editor = {de Campos, Cassio and Maathuis, Marloes H.},
 month = {Jul},
 pages = {1206--1216},
 pdf = {https://proceedings.mlr.press/v161/ober21a/ober21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The promises and pitfalls of deep kernel learning},
 url = {https://proceedings.mlr.press/v161/ober21a.html},
 volume = {161},
 year = {2021}
}

