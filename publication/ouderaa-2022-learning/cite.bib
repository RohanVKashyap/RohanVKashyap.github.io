@inproceedings{ouderaa2022learning,
 abstract = {Assumptions about invariances or symmetries in data can significantly increase the predictive power of statistical models. Many commonly used machine learning models are constraint to respect certain symmetries, such as translation equivariance in convolutional neural networks, and incorporating other symmetry types is actively being studied. Yet, learning invariances from the data itself remains an open research problem. It has been shown that the marginal likelihood offers a principled way to learn invariances in Gaussian Processes. We propose a weight-space equivalent to this approach, by minimizing a lower bound on the marginal likelihood to learn invariances in neural networks, resulting in naturally higher performing models.},
 author = {van der Ouderaa, Tycho F.A. and van der Wilk, Mark},
 booktitle = {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence (UAI)},
 editor = {Cussens, James and Zhang, Kun},
 month = {Aug},
 pages = {1992--2001},
 pdf = {https://proceedings.mlr.press/v180/ouderaa22a/ouderaa22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning invariant weights in neural networks},
 url = {https://proceedings.mlr.press/v180/ouderaa22a.html},
 volume = {180},
 year = {2022}
}

