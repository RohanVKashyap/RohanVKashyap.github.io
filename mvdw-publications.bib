@inproceedings{gal2014parallel,
title = {Distributed Variational Inference in Sparse {G}aussian Process Regression and Latent Variable Models},
author = {Yarin Gal and Mark van der Wilk and Carl Edward Rasmussen},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3257--3265},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5593-distributed-variational-inference-in-sparse-gaussian-process-regression-and-latent-variable-models.pdf}
}

@misc{galvdw2014tutorial,
Author = {Yarin Gal and Mark van der Wilk},
Title = {Variational Inference in Sparse {G}aussian Process Regression and Latent Variable Models - a Gentle Tutorial},
Year = {2014},
Eprint = {arXiv:1402.1412},
url = {https://arxiv.org/abs/1402.1412},
}

@incollection{vdw2014wplvm,
author = {Mark van der Wilk and Andrew Gordon Wilson and Carl Edward Rasmussen},
title = {Variational Inference for Latent Variable Modelling of Correlation Structure},
booktitle = {NIPS 2014 Workshop of Advances in Variational Inference},
year = {2014},
url = {https://drive.google.com/file/d/0BwY-r_90KHY4Vmd4UzBhbEI0RWM/view?usp=sharing},
}

@inproceedings{bauer2016understanding,
title = {Understanding Probabilistic Sparse {G}aussian Process Approximations},
author = {Bauer, Matthias and van der Wilk, Mark and Rasmussen, Carl Edward},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {1533--1541},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6477-understanding-probabilistic-sparse-gaussian-process-approximations.pdf}
}

@incollection{mcallister2016exploration,
title = {Data-Efficient Policy Search using {PILCO} and Directed-Exploration},
author = {Rowan McAllister and Mark van der Wilk and Carl Edward Rasmussen},
booktitle = {ICML 2016 Workshop on Data-Efficient Machine Learning},
year = {2016},
url = {https://drive.google.com/open?id=0B0VXvxUNyiVSQ01VUXIzdXQ4MW8},
}

@article{gpflow,
  author  = {Alexander G. de G. Matthews and Mark van der Wilk and Tom Nickson and Keisuke Fujii and Alexis Boukouvalas and Pablo Le{{\'o}}n-Villagr{{\'a}} and Zoubin Ghahramani and James Hensman},
  title   = {{GPflow}: A {G}aussian Process Library using {T}ensor{F}low},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {40},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v18/16-537.html}
}

@inproceedings{vdw2017convgp,
title = {Convolutional {G}aussian Processes},
author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {2849--2858},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6877-convolutional-gaussian-processes.pdf}
}

@inproceedings{mcallister2017concrete,
  title={Concrete problems for autonomous vehicle safety: advantages of {B}ayesian deep learning},
  author={McAllister, Rowan and Gal, Yarin and Kendall, Alex and van der Wilk, Mark and Shah, Amar and Cipolla, Roberto and Weller, Adrian Vivian},
  year={2017},
  organization={International Joint Conferences on Artificial Intelligence},
  url={https://www.ijcai.org/Proceedings/2017/0661.pdf},
}

@incollection{ialongo2017closed,
title = {Closed-form Inference and Prediction in {G}aussian Process State-Space Models},
author = {Alessandro Davide Ialongo and Mark van der Wilk and Carl Edward Rasmussen},
booktitle = {NIPS 2017 Workshop on Time Series},
year = {2017},
url = {https://arxiv.org/abs/1812.03580},
}

@inproceedings{tran2019layers,
 author = {Tran, Dustin and Dusenberry, Mike and van der Wilk, Mark and Hafner, Danijar},
 booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Layers: A Module for Neural Network Uncertainty},
 url = {https://proceedings.neurips.cc/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{vdw2018invgp,
title = {Learning Invariances using the Marginal Likelihood},
author = {van der Wilk, Mark and Bauer, Matthias and John, ST and Hensman, James},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {9960--9970},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8199-learning-invariances-using-the-marginal-likelihood.pdf}
}

@InProceedings{burt2019rates,
title = {Rates of Convergence for Sparse Variational {G}aussian Process Regression},
author = {Burt, David and Rasmussen, Carl Edward and van der Wilk, Mark},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
pages = {862--871},
year = {2019},
editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
volume = {97},
series = {Proceedings of Machine Learning Research},
month = {Jun},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v97/burt19a/burt19a.pdf},
url = {http://proceedings.mlr.press/v97/burt19a.html},
abstract = {Excellent variational approximations to Gaussian process posteriors have been developed which avoid the $\mathcal{O}\left(N^3\right)$ scaling with dataset size $N$. They reduce the computational cost to $\mathcal{O}\left(NM^2\right)$, with $M\ll N$ the number of inducing variables, which summarise the process. While the computational cost seems to be linear in $N$, the true complexity of the algorithm depends on how $M$ must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, $M=\mathcal{O}(\log^D N)$ suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase $M$ in continual learning scenarios.}
}

@InProceedings{ialongo2019overcoming,
title = {Overcoming Mean-Field Approximations in Recurrent {G}aussian Process Models},
author = {Ialongo, Alessandro Davide and van der Wilk, Mark and Hensman, James and Rasmussen, Carl Edward},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
pages = {2931--2940},
year = {2019},
editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
volume = {97},
series = {Proceedings of Machine Learning Research},
month = {Jun},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v97/ialongo19a/ialongo19a.pdf},
url = {http://proceedings.mlr.press/v97/ialongo19a.html},
abstract = {We identify a new variational inference scheme for dynamical systems whose transition function is modelled by a Gaussian process. Inference in this setting has either employed computationally intensive MCMC methods, or relied on factorisations of the variational posterior. As we demonstrate in our experiments, the factorisation between latent system states and transition function can lead to a miscalibrated posterior and to learning unnecessarily large noise terms. We eliminate this factorisation by explicitly modelling the dependence between state trajectories and the low-rank representation of our Gaussian process posterior. Samples of the latent states can then be tractably generated by conditioning on this representation. The method we obtain gives better predictive performance and more calibrated estimates of the transition function, yet maintains the same time and space complexities as mean-field methods.}
}

@phdthesis{vdw2019sparse,
  title={Sparse Gaussian process approximations and applications},
  author={Van der Wilk, Mark},
  year={2019},
  school={University of Cambridge},
  url={https://www.repository.cam.ac.uk/handle/1810/288347},
}

@inproceedings{heaukulani2019wishart,
 author = {Heaukulani, Creighton and van der Wilk, Mark},
 booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes},
 url = {https://proceedings.neurips.cc/paper/2019/file/5b168fdba5ee5ea262cc2d4c0b457697-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{vdw2020inv,
title = {Variational Gaussian Process Models without Matrix Inverses},
author = {van der Wilk, Mark and John, ST and Artemev, Artem and Hensman, James},
booktitle = {Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference},
pages = {1--9},
year = {2020},
editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
volume = {118},
series = {Proceedings of Machine Learning Research},
month = {Jan},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v118/wilk20a/wilk20a.pdf},
url = {http://proceedings.mlr.press/v118/wilk20a.html},
abstract = {In this work, we provide a variational lower bound that can be computed without expensive matrix operations like inversion. Our bound can be used as a drop-in replacement to the existing variational method of Hensman et al. (2013, 2015), and can therefore directly be applied in a wide variety of models, such as deep GPs (Damianou and Lawrence, 2013). We focus on the theoretical properties of this new bound, and show some initial experimental results for optimising this bound. We hope to realise the full promise in scalability that this new bound has in future work.}
}

@incollection{burt2020understanding,
title={Understanding Variational Inference in Function-Space},
author={David R. Burt and Sebastian W. Ober and Adri{\`a} Garriga-Alonso and Mark van der Wilk},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},
month={jan},
url = {https://arxiv.org/abs/2011.09421},
}

@misc{sedgwick2020design,
Author = {Ruby Sedgwick and John Goertz and Molly Stevens and Ruth Misener and Mark van der Wilk},
Title = {Design of Experiments for Verifying Biomolecular Networks},
Year = {2020},
Eprint = {arXiv:2011.10575},
url = {https://arxiv.org/abs/2011.10575},
}


@InProceedings{dutordoir2020dcgp,
  title = 	 {Bayesian Image Classification with Deep Convolutional Gaussian Processes},
  author =       {Dutordoir, Vincent and van der Wilk, Mark and Artemev, Artem and Hensman, James},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {1529--1539},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/dutordoir20a/dutordoir20a.pdf},
  url = 	 {http://proceedings.mlr.press/v108/dutordoir20a.html},
  abstract = 	 {In decision-making systems, it is important to have classifiers that have calibrated uncertainties, with an optimisation objective that can be used for automated model selection and training. Gaussian processes (GPs) provide uncertainty estimates and a marginal likelihood objective, but their weak inductive biases lead to inferior accuracy. This has limited their applicability in certain tasks (e.g. image classification). We propose a translation insensitive convolutional kernel, which relaxes the translation invariance constraint imposed by previous convolutional GPs. We show how we can use the marginal likelihood to learn the degree of insensitivity. We also reformulate GP image-to-image convolutional mappings as multi-output GPs, leading to deep convolutional GPs. We show experimentally that our new kernel improves performance in both single-layer and deep models. We also demonstrate that our fully Bayesian approach improves on dropout-based Bayesian deep learning methods in terms of uncertainty and marginal likelihood estimates.}
}

@misc{vdw2020framework,
Author = {Mark van der Wilk and Vincent Dutordoir and ST John and Artem Artemev and Vincent Adam and James Hensman},
Title = {A Framework for Interdomain and Multioutput Gaussian Processes},
Year = {2020},
Eprint = {arXiv:2003.01115},
url = {https://arxiv.org/abs/2003.01115},
}

@misc{lyle2020benefits,
Author = {Clare Lyle and Mark van der Wilk and Marta Kwiatkowska and Yarin Gal and Benjamin Bloem-Reddy},
Title = {On the Benefits of Invariance in Neural Networks},
Year = {2020},
Eprint = {arXiv:2005.00178},
url = {https://arxiv.org/abs/2005.00178},
}

@article{burt2020convergence,
  author  = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},
  title   = {Convergence of Sparse Variational Inference in Gaussian Processes Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {131},
  pages   = {1-63},
  url     = {http://jmlr.org/papers/v21/19-1015.html}
}

@inproceedings{monteiro2020correlated,
 author = {Monteiro, Miguel and Le Folgoc, Loic and Coelho de Castro, Daniel and Pawlowski, Nick and Marques, Bernardo and Kamnitsas, Konstantinos and van der Wilk, Mark and Glocker, Ben},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12756--12767},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty},
 url = {https://proceedings.neurips.cc/paper/2020/file/95f8d9901ca8878e291552f001f67692-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{burt2020vof,
Author = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},
Title = {Variational Orthogonal Features},
Year = {2020},
Eprint = {arXiv:2006.13170},
url = {https://arxiv.org/abs/2006.13170},
}

@misc{smith2020capsules,
Author = {Lewis Smith and Lisa Schut and Yarin Gal and Mark van der Wilk},
Title = {Capsule Networks -- A Probabilistic Perspective},
Year = {2020},
Eprint = {arXiv:2004.03553},
url = {https://arxiv.org/abs/2004.03553},
}

@inproceedings{lyle2020trainingspeed,
 author = {Lyle, Clare and Schut, Lisa and Ru, Robin and Gal, Yarin and van der Wilk, Mark},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {10396--10408},
 publisher = {Curran Associates, Inc.},
 title = {A Bayesian Perspective on Training Speed and Model Selection},
 url = {https://proceedings.neurips.cc/paper/2020/file/75a7c30fc0063c4952d7eb044a3c0897-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{fortuin2022bayesian,
title={Bayesian Neural Network Priors Revisited},
author={Vincent Fortuin and Adri{\`a} Garriga-Alonso and Sebastian W. Ober and Florian Wenzel and Gunnar Ratsch and Richard E Turner and Mark van der Wilk and Laurence Aitchison},
booktitle={International Conference on Learning Representations (ICLR)},
year={2022},
url={https://openreview.net/forum?id=xkjqJYqRJy}
}

@misc{dutordoir2021gpflux,
Author = {Vincent Dutordoir and Hugh Salimbeni and Eric Hambro and John McLeod and Felix Leibfried and Artem Artemev and Mark van der Wilk and James Hensman and Marc P. Deisenroth and ST John},
Title = {GPflux: A Library for Deep Gaussian Processes},
Year = {2021},
Eprint = {arXiv:2104.05674},
url = {https://arxiv.org/abs/2104.05674},
}

@InProceedings{ober2021promises,
  title = 	 {The promises and pitfalls of deep kernel learning},
  author =       {Ober, Sebastian W. and Rasmussen, Carl E. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = 	 {1206--1216},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/ober21a/ober21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/ober21a.html},
  abstract = 	 {Deep kernel learning and related techniques promise to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify pathological behavior, including overfitting, on a simple toy example. We explore this pathology, explaining its origins and considering how it applies to real datasets. Through careful experimentation on UCI datasets, CIFAR-10, and the UTKFace dataset, we find that the overfitting from overparameterized deep kernel learning, in which the model is “somewhat Bayesian”, can in certain scenarios be worse than that from not being Bayesian at all. However, we find that a fully Bayesian treatment of deep kernel learning can rectify this overfitting and obtain the desired performance improvements over standard neural networks and Gaussian processes.}
}

@InProceedings{garriga-alonso2021correlated,
  title = 	 {Correlated weights in infinite limits of deep convolutional neural networks},
  author =       {Garriga-Alonso, Adri\`a and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1998--2007},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/garriga-alonso21a/garriga-alonso21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/garriga-alonso21a.html},
  abstract = 	 {Infinite width limits of deep neural networks often have tractable forms. They have been used to analyse the behaviour of finite networks, as well as being useful methods in their own right. When investigating infinitely wide convolutional neural networks (CNNs), it was observed that the correlations arising from spatial weight sharing disappear in the infinite limit. This is undesirable, as spatial correlation is the main motivation behind CNNs. We show that the loss of this property is not a consequence of the infinite limit, but rather of choosing an independent weight prior. Correlating the weights maintains the correlations in the activations. Varying the amount of correlation interpolates between independent-weight limits and mean-pooling. Empirical evaluation of the infinitely wide network shows that optimal performance is achieved between the extremes, indicating that correlations can be useful.}
}

@InProceedings{artemevburt2021cglb,
  title = 	 {Tighter Bounds on the Log Marginal Likelihood of Gaussian Process Regression Using Conjugate Gradients},
  author =       {Artemev, Artem and Burt, David R. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning (ICML)},
  pages = 	 {362--372},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/artemev21a/artemev21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/artemev21a.html},
  abstract = 	 {We propose a lower bound on the log marginal likelihood of Gaussian process regression models that can be computed without matrix factorisation of the full kernel matrix. We show that approximate maximum likelihood learning of model parameters by maximising our lower bound retains many benefits of the sparse variational approach while reducing the bias introduced into hyperparameter learning. The basis of our bound is a more careful analysis of the log-determinant term appearing in the log marginal likelihood, as well as using the method of conjugate gradients to derive tight lower bounds on the term involving a quadratic form. Our approach is a step forward in unifying methods relying on lower bound maximisation (e.g. variational methods) and iterative approaches based on conjugate gradients for training Gaussian processes. In experiments, we show improved predictive performance with our model for a comparable amount of training time compared to other conjugate gradient based approaches.}
}

@InProceedings{schwoebel2022layer,
  title = 	 {Last Layer Marginal Likelihood for Invariance Learning},
  author =       {Pola Schwöbel and Martin Jørgensen and Sebastian W. Ober and Mark van der Wilk},
  booktitle = 	 {Proceedings of the Twenty Fifth International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = 	 {2022},
  url={https://arxiv.org/abs/2106.07512}
}

@InProceedings{nabarro2022data,
  title={Data augmentation in Bayesian neural networks and the cold posterior effect},
  author={Seth Nabarro and Stoil Ganev and Adrià Garriga-Alonso and Vincent Fortuin and Mark van der Wilk and Laurence Aitchison},
  booktitle = 	 {Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = 	 {2022},
  month =        {Aug},
  url={https://arxiv.org/abs/2106.05586}
}

@misc{burt2021barely,
      title={Barely Biased Learning for Gaussian Process Regression}, 
      author={David R. Burt and Artem Artemev and Mark van der Wilk},
      year={2021},
      eprint={2109.09417},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2109.09417}
}

@inproceedings{dutordoir2021relu,
 author = {Vincent Dutordoir and James Hensman and Mark van der Wilk and Carl Henrik Ek and Zoubin Ghahramani and Nicolas Durrande},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 title = {Deep Neural Networks as Point Estimates for Deep Gaussian Processes},
 url = {https://arxiv.org/abs/2105.04504},
 volume = {34},
 year = {2021}
}

@inproceedings{ru2021speed,
 author = {Binxin Ru and Clare Lyle and Lisa Schut and Miroslav Fil and Mark van der Wilk and Yarin Gal},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 title = {Speedy Performance Estimation for Neural Architecture Search},
 volume = {34},
 year = {2021},
 url = {https://arxiv.org/abs/2006.04492},
}


@InProceedings{ouderaa2022learning,
  title = 	 {Learning invariant weights in neural networks},
  author =       {van der Ouderaa, Tycho F.A. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = 	 {1992--2001},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/ouderaa22a/ouderaa22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/ouderaa22a.html},
  abstract = 	 {Assumptions about invariances or symmetries in data can significantly increase the predictive power of statistical models. Many commonly used machine learning models are constraint to respect certain symmetries, such as translation equivariance in convolutional neural networks, and incorporating other symmetry types is actively being studied. Yet, learning invariances from the data itself remains an open research problem. It has been shown that the marginal likelihood offers a principled way to learn invariances in Gaussian Processes. We propose a weight-space equivalent to this approach, by minimizing a lower bound on the marginal likelihood to learn invariances in neural networks, resulting in naturally higher performing models.}
}

@incollection{vdw2022inv,
title={Improved Inverse-Free Variational Bounds for Sparse Gaussian Processes},
author={Mark van der Wilk and Artem Artemev and James Hensman},
booktitle={Fourth Symposium on Advances in Approximate Bayesian Inference},
year={2022},
month={feb},
url={https://openreview.net/forum?id=t2vxi9fNhKu},
}

@incollection{popescu2022tproc,
title={Matrix Inversion free variational inference in Conditional Student's T Processes},
author={Sebastian Popescu and Ben Glocker and Mark van der Wilk},
booktitle={Fourth Symposium on Advances in Approximate Bayesian Inference},
year={2022},
month={feb},
url={https://openreview.net/forum?id=jLLR71k9Hsi},
}

@inproceedings{artemev2022xla,
  doi = {10.48550/ARXIV.2206.14148},
  url = {https://arxiv.org/abs/2206.14148},
  author = {Artemev, Artem and Roeder, Tilman and van der Wilk, Mark},
  title = {Memory Safe Computations with XLA Compiler},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {35},
  year = {2022},
  month = {Dec},
}

@inproceedings{immerouderaa2022deepinv,
  doi = {10.48550/ARXIV.2202.10638},
  url = {https://arxiv.org/abs/2202.10638},
  author = {Immer, Alexander and van der Ouderaa, Tycho F. A. and Rätsch, Gunnar and Fortuin, Vincent and van der Wilk, Mark},
  title = {Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {35},
  year = {2022},
  month = {Dec},
}

@inproceedings{folch2022snake,
  doi = {10.48550/ARXIV.2202.00060},
  url = {https://arxiv.org/abs/2202.00060},
  author = {Folch, Jose Pablo and Zhang, Shiqiang and Lee, Robert M and Shafei, Behrang and Walz, David and Tsay, Calvin and van der Wilk, Mark and Misener, Ruth},
  title = {SnAKe: Bayesian Optimization with Pathwise Exploration},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {35},
  year = {2022},
  month = {Dec},
}

@inproceedings{ouderaa2022relaxing,
  doi = {10.48550/ARXIV.2204.07178},
  url = {https://arxiv.org/abs/2204.07178},
  author = {van der Ouderaa, Tycho F. A. and Romero, David W. and van der Wilk, Mark},
  title = {Relaxing Equivariance Constraints with Non-stationary Continuous Filters},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {35},
  year = {2022},
  month = {Dec},
}
